"""Generate answers with local models.

Usage:
python3 gen_model_answer.py --model-path lmsys/fastchat-t5-3b-v1.0 --model-id fastchat-t5-3b-v1.0
"""
# adapted from fastchat: https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/gen_model_answer.py

import json
import os
import time
import torch
import numpy as np
import shortuuid

from fastchat.llm_judge.common import load_questions
from fastchat.model import get_conversation_template
from tqdm import tqdm
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "4"

def run_eval(
        model,
        tokenizer,
        forward_func,
        model_id,
        question_file,
        question_begin,
        question_end,
        answer_file,
        max_new_tokens,
        num_choices,
        num_gpus_per_model,
        num_gpus_total,
        **kwargs,
):
    questions = load_questions(question_file, question_begin, question_end)

    # Split the question file into `num_gpus` files
    assert num_gpus_total % num_gpus_per_model == 0
    use_ray = num_gpus_total // num_gpus_per_model > 1

    if use_ray:
        import ray
        ray.init()
        get_answers_func = ray.remote(num_gpus=num_gpus_per_model)(
            get_model_answers
        ).remote
    else:
        get_answers_func = get_model_answers

    chunk_size = len(questions) // (num_gpus_total // num_gpus_per_model)  # // 2
    ans_handles = []
    for i in range(0, len(questions), chunk_size):
        ans_handles.append(
            get_answers_func(
                model,
                tokenizer,
                forward_func,
                model_id,
                questions[i: i + chunk_size],
                answer_file,
                max_new_tokens,
                num_choices,
                **kwargs,
            )
        )

    if use_ray:
        ray.get(ans_handles)



@torch.inference_mode()
def get_model_answers(
    model,
    tokenizer,
    forward_func,
    model_id,
    questions,
    answer_file,
    max_new_tokens,
    num_choices,
    **kwargs,
):
    model.eval()
    print('Check model training state:', model.training)
    
    # ç¡®ä¿ tokenizer è®¾ç½®äº† padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    print(f"DEBUG: Using Chat Template: {tokenizer.chat_template is not None}")

    # ================= Warmup =================
    print("Starting Warmup...")
    for _ in range(3):
        torch.manual_seed(0)
        # æ„å»ºä¸€ä¸ªç®€å•çš„ç¬¦åˆ Llama-3 æ ¼å¼çš„ warmup è¾“å…¥
        warmup_msgs = [{"role": "user", "content": "Hello"}]
        
        # ä½¿ç”¨ apply_chat_template æˆ– fallback
        try:
            if tokenizer.chat_template:
                input_ids = tokenizer.apply_chat_template(
                    warmup_msgs, 
                    add_generation_prompt=True, 
                    return_tensors="pt", 
                ).to(model.device)
            else:
                # Fallback: æ‰‹åŠ¨æ„å»º Llama-3 æ ¼å¼
                text = "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nHello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
                input_ids = tokenizer.encode(text, return_tensors="pt").to(model.device)
        except Exception as e:
            print(f"Tokenization failed: {e}")
            # ç®€å•çš„ fallback
            input_ids = torch.tensor([[1, 2, 3]], device=model.device)
        
        inputs = {
            "input_ids": input_ids,
            "attention_mask": torch.ones_like(input_ids)
        }
        
        try:
            torch.cuda.synchronize()
            # è°ƒç”¨ forward_func
            forward_func(
                inputs,  # ä¼ å…¥å­—å…¸æ ¼å¼
                model,
                tokenizer,
                max_new_tokens=20,
                **kwargs,
            )
            torch.cuda.synchronize()
        except Exception as e:
            print(f"Warmup failed: {e}")
    print('Warmup done')

    # ================= Main Loop =================
    accept_lengths_tree = []
    
    for question in tqdm(questions):
        choices = []
        for i in range(num_choices):
            cur_accept_lengths_tree = []
            torch.manual_seed(i)
            
            messages = [] 
            turns = []
            steps = []
            new_tokens = []
            wall_time = []
            
            for j in range(len(question["turns"])):
                qs = question["turns"][j]
                
                # âœ… æ¯ä¸ª turn éƒ½é‡æ–°æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²
                messages = []
                # æ·»åŠ ä¹‹å‰çš„æ‰€æœ‰è½®æ¬¡
                for k in range(j):
                    messages.append({"role": "user", "content": question["turns"][k]})
                    messages.append({"role": "assistant", "content": turns[k]})
                # æ·»åŠ å½“å‰é—®é¢˜
                messages.append({"role": "user", "content": qs})
                
                # æ„å»ºè¾“å…¥
                try:
                    if tokenizer.chat_template:
                        input_ids = tokenizer.apply_chat_template(
                            messages, 
                            add_generation_prompt=True, 
                            return_tensors="pt",
                        ).to(model.device)
                    else:
                        # Fallback: æ‰‹åŠ¨æ„å»ºå¯¹è¯æ ¼å¼
                        test_text = "test"
                        test_ids = tokenizer.encode(test_text, add_special_tokens=True)
                        has_auto_bos = test_ids[0] == tokenizer.bos_token_id if tokenizer.bos_token_id else False
                        
                        text = "" if has_auto_bos else "<|begin_of_text|>"
                        for msg in messages:
                            text += f"<|start_header_id|>{msg['role']}<|end_header_id|>\n\n{msg['content']}<|eot_id|>"
                        text += "<|start_header_id|>assistant<|end_header_id|>\n\n"
                        input_ids = tokenizer.encode(text, return_tensors="pt", add_special_tokens=has_auto_bos).to(model.device)
                        
                    # ğŸ” æ·»åŠ è°ƒè¯•è¾“å‡º
                    if j == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªturnæ‰“å°
                        print(f"\n{'='*80}")
                        print(f"Question ID: {question['question_id']}")
                        print(f"Question: {qs[:100]}...")
                        decoded = tokenizer.decode(input_ids[0], skip_special_tokens=False)
                        print(f"Tokenized prompt (first 200 chars): {decoded[:200]}...")
                        print(f"{'='*80}\n")
                        
                except Exception as e:
                    print(f"Tokenization failed for question {question['question_id']}: {e}")
                    # ç®€å•çš„ fallback
                    input_ids = torch.tensor([[1, 2, 3]], device=model.device)
                
                model_inputs = {
                    "input_ids": input_ids,
                    "attention_mask": torch.ones_like(input_ids)
                }
                
                input_ids_len = model_inputs['input_ids'].shape[1]
                
                output_str = "ERROR"
                step = 0
                new_token = 0
                accept_length_tree = []
                total_time = 0.0

                try:
                    torch.cuda.synchronize()
                    start_time = time.time()
                    
                    # è°ƒç”¨æ¨ç†å‡½æ•°
                    output_ids, new_token, step, accept_length_tree = forward_func(
                        model_inputs,
                        model,
                        tokenizer,
                        max_new_tokens,
                        **kwargs,
                    )
                    
                    torch.cuda.synchronize()
                    total_time = time.time() - start_time
                    
                    # å¤„ç†è¾“å‡º
                    if isinstance(output_ids, torch.Tensor):
                        generated_ids = output_ids[0][input_ids_len:] 
                        output_str = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
                    else:
                        print(f"Warning: output_ids is not Tensor, type: {type(output_ids)}")
                        output_str = "ERROR"

                except RuntimeError as e:
                    print(f"ERROR question ID: {question['question_id']}, Error: {e}")
                    output_str = "ERROR"
                    import traceback
                    traceback.print_exc()
                
                turns.append(output_str)
                steps.append(int(step))
                new_tokens.append(int(new_token))
                wall_time.append(total_time)
                cur_accept_lengths_tree.extend(accept_length_tree)

            choices.append({
                "index": i, 
                "turns": turns, 
                "decoding_steps": steps, 
                "new_tokens": new_tokens, 
                "wall_time": wall_time,
                "accept_lengths": cur_accept_lengths_tree
            })
            
            accept_lengths_tree.extend(cur_accept_lengths_tree)

        os.makedirs(os.path.dirname(answer_file), exist_ok=True)
        with open(os.path.expanduser(answer_file), "a") as fout:
            ans_json = {
                "question_id": question["question_id"],
                "category": question["category"],
                "answer_id": shortuuid.uuid(),
                "model_id": model_id,
                "choices": choices,
                "tstamp": time.time(),
            }
            fout.write(json.dumps(ans_json) + "\n")

    if len(accept_lengths_tree) > 0:
        print("#Mean accepted tokens: ", np.mean(accept_lengths_tree))
    else:
        print("#Mean accepted tokens: N/A")


def reorg_answer_file(answer_file):
    """Sort by question id and de-duplication"""
    answers = {}
    with open(answer_file, "r") as fin:
        for l in fin:
            qid = json.loads(l)["question_id"]
            answers[qid] = l

    qids = sorted(list(answers.keys()))
    with open(answer_file, "w") as fout:
        for qid in qids:
            fout.write(answers[qid])
